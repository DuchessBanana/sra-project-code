# -*- coding: utf-8 -*-
"""Final Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sUxspIBzfsmN6s_yHP8r6c5K7iKR-nUU

# **Final Project: Predicting IGBT Failure Due To Thermal Overstress**
By Sanatan Mishra, Mahika Maini, and Aidan Chan
"""

!pip install scikeras

!pip install google.colab

from google.colab import files
import scipy.io
import pandas as pd
import numpy as np
import datetime
from numpy.lib import recfunctions
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow import keras
from scikeras.wrappers import KerasRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import *
import math

# Seed value
# Apparently you may use different seed values at each stage
#seed_value= 13 # 7.1% error on best
#seed_value= 280 # almost same error as baseline
#seed_value= 278
#seed_value= 231
seed_value= None

# 1. Set `PYTHONHASHSEED` environment variable at a fixed value
import os
os.environ['PYTHONHASHSEED']=str(seed_value)

# 2. Set `python` built-in pseudo-random generator at a fixed value
import random
random.seed(seed_value)

# 3. Set `numpy` pseudo-random generator at a fixed value
import numpy as np
np.random.seed(seed_value)

# 4. Set the `tensorflow` pseudo-random generator at a fixed value
import tensorflow as tf
tf.random.set_seed(seed_value)
# for later versions:
# tf.compat.v1.set_random_seed(seed_value)

# 5. Configure a new global `tensorflow` session
from keras import backend as K
session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)
sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)
tf.compat.v1.keras.backend.set_session(sess)

"""## Converting Data to CSV"""

uploaded = files.upload()

data_set = scipy.io.loadmat('Device2  1.mat', struct_as_record=True, squeeze_me=True, mat_dtype=False)
print(data_set)
df = pd.DataFrame(data_set['measurement'])
df

mat = scipy.io.loadmat('Device2  1.mat')
mat = {k:v for k, v in mat.items() if k[0] != '_'}
df = pd.DataFrame({k: np.array(v).flatten() for k, v in mat.items()})

data_set = scipy.io.loadmat("Device2  1.mat")

arr = mat['measurement']
arr = recfunctions.repack_fields(arr)
spam = arr.tolist()
recfunctions.repack_fields(spam[0][0][0])
for a in spam:
  for b in a:
    for c in b:
      c = recfunctions.repack_fields(c)
      c = c.tolist()
      for d in c:
        for e in d:
          for f in e:
            f = recfunctions.repack_fields(f)
            f = f.tolist()
            for g in f:
              for h in g:
                if (type(h) == tuple):
                  for i in h:
                    i = recfunctions.repack_fields(i)
                    i = i.tolist()
      print(c)

spam

mat['measurement'].tolist()

for i in df:
  print(i)
  print(type(df[i]))
  print(df[i].dtype)
  print(df[i].ndim)
  print(df[i].size)
  print(df[i].dtypes)

print(df[i].keys)
print(df[i].get("pwmTempControllerState"))

data = pd.DataFrame(df[i].astype('object'), columns = ["pwmTempControllerState"])
data

df.dropna()

"""## Adding Y Variable"""

uploaded = files.upload()

df = pd.read_csv('5a.csv')

for i in range(1, len(df)):
  delta_y = df.iloc[i]['supplyVoltage'] - df.iloc[i - 1]['supplyVoltage']
  if delta_y < -0.075 and i > 7000:
    print(i)
    break

last_row = i

df = df.drop(labels = range(last_row + 1, len(df)), axis = 0)

ruls = []
for i in range(len(df)):
  ruls.append(df.iloc[-1]['timeEpoch'] - df.iloc[i]['timeEpoch'])
remaining_useful_lifespans = []
for r in range(len(ruls)):
  remaining_useful_lifespans.append(ruls[r] / ruls[0])
df['remainingUsefulLifespan'] = remaining_useful_lifespans
df

df.to_csv('5a.csv')

"""## Exploratory Data Analysis"""

#files.upload()
#df = pd.read_csv('train.csv')
df = pd.read_csv('~/Desktop/final_data/train.csv')

files.upload()
df = pd.read_csv('2a.csv')

# Scatterplot showing supply voltage seems to be positively correlated with shutdown temperature
sns.scatterplot(df, x='supplyVoltage', y='shutdownTemp')

summary = df.describe(percentiles=[.01, .05, .10, .25, .50, .75, .90, .95, .99])
summary

# Distribution Plots
for k in df:
  if df.dtypes[k] != 'object':
    sns.displot(df, x = k)
    sns.displot(df, x = k, kind = "kde")

# Violin Plots
for k in df:
  if df.dtypes[k] != 'object':
    ax = plt.subplot()
    sns.violinplot(df, x = k, ax = ax)
    plt.show()

# Box Plots
for k in df:
  if df.dtypes[k] != 'object':
    ax = plt.subplot()
    sns.boxplot(df, x = k, ax = ax)
    plt.show()

# Scatterplots
for k in df:
  if df.dtypes[k] != 'object':
    for l in df:
      if k != l and df.dtypes[l] != 'object':
        ax = plt.subplot()
        ax.scatter(x = df[k], y = df[l])
        plt.xlabel(k)
        plt.ylabel(l)
        plt.show()

# Correlation matrix
corr = df.corr()
sns.heatmap(corr, annot=True)

corr = df[['supplyVoltage', 'node1Voltage', 'node2Voltage', 'collectorEmitterCurrent', 'packageTemperature', 'remainingUsefulLifespan']].corr()
sns.set(font_scale=1.3)
sns.heatmap(corr, annot=True)

# Time Series Plot
ax = plt.subplot()
ax.scatter(x = df['timeEpoch'], y = df['supplyVoltage'], c = [[0, 0, 1]])
ax.scatter(x = df['timeEpoch'], y = df['node1Voltage'], c = [[1, 0, 0]])
ax.scatter(x = df['timeEpoch'], y = df['node2Voltage'], c = [[0, 1, 0]])
ax.scatter(x = df['timeEpoch'], y = df['collectorEmitterCurrent'], c = [[1, 1, 0]])
ax.scatter(x = df['timeEpoch'], y = df['heatSinkTemperature'], c = [[0, 1, 1]])
ax.scatter(x = df['timeEpoch'], y = df['packageTemperature'], c = [[1, 0, 1]])
#ax.scatter(x = df['timeEpoch'], y = df['internalTemperature'], c = [[0, 0, 0]])

ax = plt.subplot()
#ax.scatter(x = df['supplyVoltage'], y = df['timeEpoch'], c = [[0, 0, 1]])
ax.scatter(x = df['supplyVoltage'], y = df['node1Voltage'], c = [[1, 0, 0]])
ax.scatter(x = df['supplyVoltage'], y = df['node2Voltage'], c = [[0, 1, 0]])
ax.scatter(x = df['supplyVoltage'], y = df['collectorEmitterCurrent'], c = [[1, 1, 0]])
ax.scatter(x = df['supplyVoltage'], y = df['heatSinkTemperature'], c = [[0, 1, 1]])
ax.scatter(x = df['supplyVoltage'], y = df['packageTemperature'], c = [[1, 0, 1]])
ax.scatter(x = df['supplyVoltage'], y = df['internalTemperature'], c = [[0, 0, 0]])

"""## Data Preparation for Training"""

#df = df.sample(frac=1).reset_index(drop=True)
# Generate training and test datasets
# Normalize data where starting is 1 and ending is 0
# Loss: ordinary least squares, boundary condition, monotonic decreasing
features = [
        'packageTemperature', # Shows the thermal stresses placed on the IGBT
        'supplyVoltage', # Shows the supply voltage, proxy for 2 other variables
        'collectorEmitterCurrent', # Interestingly distributed,
        'node1Voltage', # Do ablation study; see which features are actually needed
        #'timeEpoch',
        'node2Voltage',
        #'heatSinkTemperature',
        #'internalTemperature'
]

X_train = df[features]
y_train = df['remainingUsefulLifespan']
# To check resilience of model arch on different patterns, do randomized battery usage data
# Try having only 1 CSV and having multiple CSVs, see which does better; quantify differences between loading curves??; does ML model do a good job predicting on that loading curve
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, shuffle=False)

#files.upload()
#test = pd.read_csv('5a.csv')
test = pd.read_csv('~/Desktop/final_data/5a.csv')
X_test = test[features]
y_test = test['remainingUsefulLifespan']

X_train

print(y_train)

print(X_test)

"""## Baseline Model"""

baseline = LinearRegression()
baseline.fit(X_train, y_train)
y_test_linear_hat = baseline.predict(X_test)
print(y_test_linear_hat)
print(math.sqrt(mean_squared_error(y_test, y_test_linear_hat)))

"""## Neural Network"""

def custom_mse(y_true, y_pred):

    false_positive_weight = 1.0
    false_negative_weight = 2.0

    # Calculate binary cross entropy
    bce = tf.keras.losses.MeanSquaredError()

    # Calculate loss
    loss = bce(y_true, y_pred)

    # Calculate weighted loss
    weighted_loss = tf.where(tf.greater(y_pred, y_true), false_negative_weight * loss, false_positive_weight * loss)

    return tf.reduce_mean(weighted_loss)

    loss = loss * [0.8]         # (batch_size, 2)

    # summing both loss values along batch dimension
    loss = K.sum(loss, axis=1)        # (batch_size,)

    return loss

def custom_mse_tempered(y_true, y_pred):

    false_positive_weight = 1.0
    false_negative_weight = 1.5

    # Calculate binary cross entropy
    bce = tf.keras.losses.MeanSquaredError()

    # Calculate loss
    loss = bce(y_true, y_pred)

    # Calculate weighted loss
    weighted_loss = tf.where(tf.greater(y_pred, y_true), false_negative_weight * loss, false_positive_weight * loss)

    return tf.reduce_mean(weighted_loss)

    loss = loss * [0.8]         # (batch_size, 2)

    # summing both loss values along batch dimension
    loss = K.sum(loss, axis=1)        # (batch_size,)

    return loss

def custom_mse_sqrted(y_true, y_pred):

    false_positive_weight = 1.0
    false_negative_weight = 2.0

    # Calculate binary cross entropy
    bce = tf.keras.losses.MeanSquaredError()

    # Calculate loss
    loss = bce(y_true, y_pred)

    # Calculate weighted loss
    weighted_loss = tf.where(tf.greater(y_pred, y_true), false_negative_weight * (loss ** 0.5), false_positive_weight * (loss))

    return tf.reduce_mean(weighted_loss)

    loss = loss * [0.8]         # (batch_size, 2)

    # summing both loss values along batch dimension
    loss = K.sum(loss, axis=1)        # (batch_size,)

    return loss

def placeholder_loss(y_true, y_pred):
  # Modded RMSE: punish overestimates more than underestimates
  # Maybe triple the error for overestimates
  # Also add NBTI to modded RMSE
  squared_difference = tf.square(y_true - y_pred)
  return tf.reduce_mean(squared_difference, axis=-1)

def pow(x):
  return x ** 3

def optimizer():
  # Return optimizer to use
  return keras.optimizers.Adam(lr=0.01)

def get_non_recurrent_model():
  model = keras.Sequential()
  keras.saving.get_custom_objects().update({'pow': pow})
  model.add(keras.Input(shape=(5,))) # change shape later
  model.add(keras.layers.Reshape((1,5)))
  model.add(keras.layers.Conv1D(filters=8, kernel_size=3, padding='same', activation=None))
  model.add(keras.layers.MaxPooling1D(pool_size=3, strides=2, padding='same'))
  model.add(keras.layers.Dense(4, activation='sigmoid'))
  model.add(keras.layers.Dense(4, activation='sigmoid'))
  model.add(keras.layers.Dense(1, activation='pow'))
  model.compile(optimizer=optimizer(), loss=custom_mse_tempered, metrics=None)
  return model

non_rnn_model = KerasRegressor(build_fn=get_non_recurrent_model, epochs=60, batch_size=5)
history_no_rnn = non_rnn_model.fit(X_train, y_train, validation_split=0.0)
y_test_no_rnn_hat = non_rnn_model.predict(X_test)
print(y_test)
print(y_test_no_rnn_hat)
print(math.sqrt(mean_squared_error(y_test, y_test_no_rnn_hat)))

def get_non_convolutional_model():
  model = keras.Sequential()
  keras.saving.get_custom_objects().update({'pow': pow})
  model.add(keras.Input(shape=(5,)))
  model.add(keras.layers.Reshape((1,5)))
  model.add(keras.layers.GRU(4, activation='sigmoid', recurrent_activation='sigmoid', return_sequences=True))
  model.add(keras.layers.GRU(4, activation='sigmoid', recurrent_activation='sigmoid'))
  #model.add(keras.layers.Dropout(0.7))
  model.add(keras.layers.Dense(1, activation='pow'))
  model.compile(optimizer=optimizer(), loss=custom_mse_tempered, metrics=None)
  return model

non_cnn_model = KerasRegressor(build_fn=get_non_recurrent_model, epochs=60, batch_size=5)
history = non_cnn_model.fit(X_train, y_train, validation_split=0.0)
y_test_no_cnn_hat = non_cnn_model.predict(X_test)
print(y_test)
print(y_test_no_cnn_hat)
print(math.sqrt(mean_squared_error(y_test, y_test_no_cnn_hat)))

def get_model():
  model = keras.Sequential()
  keras.saving.get_custom_objects().update({'pow': pow})
  model.add(keras.Input(shape=(5,)))
  model.add(keras.layers.Reshape((1,5)))
  model.add(keras.layers.Conv1D(filters=9, kernel_size=3, padding='same', activation='sigmoid'))
  model.add(keras.layers.MaxPooling1D(pool_size=3, strides=2, padding='same'))
  model.add(keras.layers.GRU(4, activation='sigmoid', recurrent_activation='sigmoid', return_sequences=True))
  model.add(keras.layers.GRU(4, activation='sigmoid', recurrent_activation='sigmoid'))
  #model.add(keras.layers.Dropout(0.7))
  model.add(keras.layers.Dense(1, activation='pow'))
  model.compile(optimizer=optimizer(), loss=custom_mse_sqrted, metrics=None)
  return model

model = KerasRegressor(build_fn=get_model, epochs=60, batch_size=5)
history = model.fit(X_train, y_train, validation_split=0.0)
y_test_hat = model.predict(X_test)
print(y_test)
print(y_test_hat)
print(math.sqrt(mean_squared_error(y_test, y_test_hat)))

def get_model_tempered_custom_loss():
  model = keras.Sequential()
  keras.saving.get_custom_objects().update({'pow': pow})
  model.add(keras.Input(shape=(5,)))
  model.add(keras.layers.Reshape((1,5)))
  model.add(keras.layers.Conv1D(filters=9, kernel_size=3, padding='same', activation='sigmoid'))
  model.add(keras.layers.MaxPooling1D(pool_size=3, strides=2, padding='same'))
  model.add(keras.layers.GRU(4, activation='sigmoid', recurrent_activation='sigmoid', return_sequences=True))
  model.add(keras.layers.GRU(4, activation='sigmoid', recurrent_activation='sigmoid'))
  #model.add(keras.layers.Dropout(0.7))
  model.add(keras.layers.Dense(1, activation='pow'))
  model.compile(optimizer=optimizer(), loss=custom_mse_tempered, metrics=None)
  return model

custom_tempered_model = KerasRegressor(build_fn=get_model_tempered_custom_loss, epochs=60, batch_size=5)
history = custom_tempered_model.fit(X_train, y_train, validation_split=0.0)
y_test_tempered_hat = custom_tempered_model.predict(X_test)
print(y_test)
print(y_test_tempered_hat)
print(math.sqrt(mean_squared_error(y_test, y_test_tempered_hat)))

plt.plot(range(len(y_test)), y_test_tempered_hat)
plt.plot(range(len(y_test)), y_test_custom_hat)
plt.plot(range(len(y_test)), y_test)

def get_model_custom_loss():
  model = keras.Sequential()
  keras.saving.get_custom_objects().update({'pow': pow})
  model.add(keras.Input(shape=(5,)))
  model.add(keras.layers.Reshape((1,5)))
  model.add(keras.layers.Conv1D(filters=9, kernel_size=3, padding='same', activation='sigmoid'))
  model.add(keras.layers.MaxPooling1D(pool_size=3, strides=2, padding='same'))
  model.add(keras.layers.GRU(4, activation='sigmoid', recurrent_activation='sigmoid', return_sequences=True))
  model.add(keras.layers.GRU(4, activation='sigmoid', recurrent_activation='sigmoid'))
  #model.add(keras.layers.Dropout(0.7))
  model.add(keras.layers.Dense(1, activation='pow'))
  model.compile(optimizer=optimizer(), loss=custom_mse, metrics=None)
  return model

custom_model = KerasRegressor(build_fn=get_model_custom_loss, epochs=60, batch_size=5)
history = custom_model.fit(X_train, y_train, validation_split=0.0)
y_test_custom_hat = custom_model.predict(X_test)
print(y_test)
print(y_test_custom_hat)
print(math.sqrt(mean_squared_error(y_test, y_test_custom_hat)))

def get_model_mse_loss():
  model = keras.Sequential()
  keras.saving.get_custom_objects().update({'pow': pow})
  model.add(keras.Input(shape=(5,)))
  model.add(keras.layers.Reshape((1,5)))
  model.add(keras.layers.Conv1D(filters=9, kernel_size=3, padding='same', activation='sigmoid'))
  model.add(keras.layers.MaxPooling1D(pool_size=3, strides=2, padding='same'))
  model.add(keras.layers.GRU(4, activation='sigmoid', recurrent_activation='sigmoid', return_sequences=True))
  model.add(keras.layers.GRU(4, activation='sigmoid', recurrent_activation='sigmoid'))
  #model.add(keras.layers.Dropout(0.7))
  model.add(keras.layers.Dense(1, activation='pow'))
  model.compile(optimizer=optimizer(), loss=placeholder_loss, metrics=None)
  return model

mse_model = KerasRegressor(build_fn=get_model_mse_loss, epochs=60, batch_size=5)
history = mse_model.fit(X_train, y_train, validation_split=0.0)
y_test_mse_hat = mse_model.predict(X_test)
print(y_test)
print(y_test_mse_hat)
print(math.sqrt(mean_squared_error(y_test, y_test_mse_hat)))

def get_model_sigmoid_out():
  model = keras.Sequential()
  keras.saving.get_custom_objects().update({'pow': pow})
  model.add(keras.Input(shape=(5,)))
  model.add(keras.layers.Reshape((1,5)))
  model.add(keras.layers.Conv1D(filters=9, kernel_size=3, padding='same', activation='sigmoid'))
  model.add(keras.layers.MaxPooling1D(pool_size=3, strides=2, padding='same'))
  model.add(keras.layers.GRU(4, activation='sigmoid', recurrent_activation='sigmoid', return_sequences=True))
  model.add(keras.layers.GRU(4, activation='sigmoid', recurrent_activation='sigmoid'))
  #model.add(keras.layers.Dropout(0.7))
  model.add(keras.layers.Dense(1, activation='sigmoid'))
  model.compile(optimizer=optimizer(), loss=custom_mse_tempered, metrics=None)
  return model

model = KerasRegressor(build_fn=get_model_sigmoid_out, epochs=60, batch_size=5)
history = model.fit(X_train, y_train, validation_split=0.0)
y_test_sigmoid_hat = model.predict(X_test)
print(y_test)
print(y_test_sigmoid_hat)
print(math.sqrt(mean_squared_error(y_test, y_test_sigmoid_hat)))

def get_model_relu_out():
  model = keras.Sequential()
  keras.saving.get_custom_objects().update({'pow': pow})
  model.add(keras.Input(shape=(5,)))
  model.add(keras.layers.Reshape((1,5)))
  model.add(keras.layers.Conv1D(filters=9, kernel_size=3, padding='same', activation='sigmoid'))
  model.add(keras.layers.MaxPooling1D(pool_size=3, strides=2, padding='same'))
  model.add(keras.layers.GRU(4, activation='sigmoid', recurrent_activation='sigmoid', return_sequences=True))
  model.add(keras.layers.GRU(4, activation='sigmoid', recurrent_activation='sigmoid'))
  #model.add(keras.layers.Dropout(0.7))
  model.add(keras.layers.Dense(1, activation='relu'))
  model.compile(optimizer=optimizer(), loss=custom_mse_tempered, metrics=None)
  return model

model = KerasRegressor(build_fn=get_model_relu_out, epochs=60, batch_size=5)
history = model.fit(X_train, y_train, validation_split=0.0)
y_test_relu_hat = model.predict(X_test)
print(y_test)
print(y_test_relu_hat)
print(math.sqrt(mean_squared_error(y_test, y_test_relu_hat)))

y_test_relu_hat = model.predict(X_test)
print(y_test)
print(y_test_relu_hat)
print(math.sqrt(mean_squared_error(y_test, y_test_relu_hat)))

plt.plot(range(len(y_test)), y_test_tempered_hat, 'fuchsia', label='Cubic Activation')
plt.plot(range(len(y_test)), y_test_sigmoid_hat, 'indigo', label='Sigmoid Activation')
plt.plot(range(len(y_test)), y_test_relu_hat, 'darkcyan', label='ReLU Activation')
plt.plot(range(len(y_test)), y_test, 'y-', label='Actual')
plt.xlabel('Time', fontweight='bold', fontsize='large')
plt.ylabel('Remaining Useful Lifespan', fontweight='bold', fontsize='large')
#plt.title('Remaining Useful Lifespan Over Time By Final Activation Function')
plt.legend(bbox_to_anchor=(1, 1))

plt.plot(range(len(y_test)), y_test_tempered_hat, 'darkcyan', label='BMSE Loss, b = 1.5')
plt.plot(range(len(y_test)), y_test_custom_hat, 'indigo', label='BMSE Loss, b = 2.0')
plt.plot(range(len(y_test)), y_test_mse_hat, 'crimson', label='MSE Loss')
plt.plot(range(len(y_test)), y_test_hat, 'dodgerblue', label='RBMSE Loss')
plt.plot(range(len(y_test)), y_test, 'y', label='Actual')
plt.xlabel('Time', fontweight='bold', fontsize='large')
plt.ylabel('Remaining Useful Lifespan', fontweight='bold', fontsize='large')
#plt.title('Remaining Useful Lifespan Over Time By Loss Function')
plt.legend(bbox_to_anchor=(1.00, 1))

plt.plot(range(len(y_test)), y_test_tempered_hat, 'dodgerblue', label='CNN + GRU', linewidth=0.5)
plt.plot(range(len(y_test)), y_test_no_cnn_hat, 'k', label='GRU-only')
plt.plot(range(len(y_test)), y_test_no_rnn_hat, 'darkcyan', label='Convolution-only')
plt.plot(range(len(y_test)), y_test_linear_hat, 'crimson', label='Linear Regression')
plt.plot(range(len(y_test)), y_test, 'y', label='Actual')
plt.xlabel('Time', fontweight='bold', fontsize='large')
plt.ylabel('Remaining Useful Lifespan', fontweight='bold', fontsize='large')
#plt.title('Remaining Useful Lifespan Over Time By Model Architecture')
plt.legend(bbox_to_anchor=(1, 1))

loss_points = np.linspace(-1, 1, num=200)
#axis = np.linspace(0, 1, num=200)
plt.plot(loss_points, loss_points ** 2)
#plt.plot(axis * 0, axis)
plt.xlabel('Error', fontweight='bold', fontsize='large')
plt.ylabel('MSE', fontweight='bold', fontsize='large')

loss_points_left = np.linspace(-1, 0, num=200)
loss_points_right = np.linspace(0, 1, num=200)
#axis = np.linspace(0, 1, num=200)
plt.plot(loss_points_left, loss_points_left ** 2)
plt.plot(loss_points_right, 2 * (loss_points_right ** 2))
#plt.plot(axis * 0, axis)
plt.xlabel('Error', fontweight='bold', fontsize='large')
plt.ylabel('BMSE (b = 2)', fontweight='bold', fontsize='large')

loss_points_left = np.linspace(-1, 0, num=200)
loss_points_right = np.linspace(0, 1, num=200)
#axis = np.linspace(0, 1, num=200)
plt.plot(loss_points_left, loss_points_left ** 2)
plt.plot(loss_points_right, 2 * (loss_points_right))
#plt.plot(axis * 0, axis)
plt.xlabel('Error', fontweight='bold', fontsize='large')
plt.ylabel('RBMSE (b = 2)', fontweight='bold', fontsize='large')

m = get_model()
print(m.summary())

m = get_non_recurrent_model()
print(m.summary())

m = get_non_convolutional_model()
print(m.summary())

y_train_hat = model.predict(X_train)
plt.plot(range(len(y_train)), y_train_hat)
plt.plot(range(len(y_train)), y_train)

vars = history.history_
plt.plot(range(1, len(vars['loss']) + 1), vars['loss'])
plt.plot(range(1, len(vars['val_loss']) + 1), vars['val_loss'])
plt.xlabel('# of Epochs')
plt.ylabel('Loss')

def line(x):
  return x ** 1

def get_line_model():
  model = keras.Sequential()
  keras.saving.get_custom_objects().update({'pow': line})
  model.add(keras.Input(shape=(5,)))
  model.add(keras.layers.Reshape((1,5)))
  model.add(keras.layers.Conv1D(filters=9, kernel_size=3, padding='same', activation='sigmoid'))
  model.add(keras.layers.MaxPooling1D(pool_size=3, strides=2, padding='same'))
  model.add(keras.layers.GRU(4, activation='sigmoid', recurrent_activation='sigmoid', return_sequences=True))
  model.add(keras.layers.GRU(4, activation='sigmoid', recurrent_activation='sigmoid'))
  #model.add(keras.layers.Dropout(0.7))
  model.add(keras.layers.Dense(1, activation='pow'))
  model.compile(optimizer=optimizer(), loss=custom_mse_tempered, metrics=None)
  return model

lin_model = KerasRegressor(build_fn=get_line_model, epochs=60, batch_size=5)
history = lin_model.fit(X_train, y_train, validation_split=0.0)
y_test_line_hat = lin_model.predict(X_test)
print(y_test)
print(y_test_line_hat)
print(math.sqrt(mean_squared_error(y_test, y_test_line_hat)))

def sq(x):
  return x ** 2

def get_sq_model():
  model = keras.Sequential()
  keras.saving.get_custom_objects().update({'pow': sq})
  model.add(keras.Input(shape=(5,)))
  model.add(keras.layers.Reshape((1,5)))
  model.add(keras.layers.Conv1D(filters=9, kernel_size=3, padding='same', activation='sigmoid'))
  model.add(keras.layers.MaxPooling1D(pool_size=3, strides=2, padding='same'))
  model.add(keras.layers.GRU(4, activation='sigmoid', recurrent_activation='sigmoid', return_sequences=True))
  model.add(keras.layers.GRU(4, activation='sigmoid', recurrent_activation='sigmoid'))
  #model.add(keras.layers.Dropout(0.7))
  model.add(keras.layers.Dense(1, activation='pow'))
  model.compile(optimizer=optimizer(), loss=custom_mse_tempered, metrics=None)
  return model

sq_model = KerasRegressor(build_fn=get_sq_model, epochs=60, batch_size=5)
history = sq_model.fit(X_train, y_train, validation_split=0.0)
y_test_sq_hat = sq_model.predict(X_test)
print(y_test)
print(y_test_sq_hat)
print(math.sqrt(mean_squared_error(y_test, y_test_sq_hat)))

def quad(x):
  return x ** 4

def get_quad_model():
  model = keras.Sequential()
  keras.saving.get_custom_objects().update({'pow': quad})
  model.add(keras.Input(shape=(5,)))
  model.add(keras.layers.Reshape((1,5)))
  model.add(keras.layers.Conv1D(filters=9, kernel_size=3, padding='same', activation='sigmoid'))
  model.add(keras.layers.MaxPooling1D(pool_size=3, strides=2, padding='same'))
  model.add(keras.layers.GRU(4, activation='sigmoid', recurrent_activation='sigmoid', return_sequences=True))
  model.add(keras.layers.GRU(4, activation='sigmoid', recurrent_activation='sigmoid'))
  #model.add(keras.layers.Dropout(0.7))
  model.add(keras.layers.Dense(1, activation='pow'))
  model.compile(optimizer=optimizer(), loss=custom_mse_tempered, metrics=None)
  return model

quad_model = KerasRegressor(build_fn=get_quad_model, epochs=60, batch_size=5)
history = quad_model.fit(X_train, y_train, validation_split=0.0)
y_test_quad_hat = quad_model.predict(X_test)
print(y_test)
print(y_test_quad_hat)
print(math.sqrt(mean_squared_error(y_test, y_test_quad_hat)))

def quint(x):
  return x ** 5

def get_quint_model():
  model = keras.Sequential()
  keras.saving.get_custom_objects().update({'pow': quint})
  model.add(keras.Input(shape=(5,)))
  model.add(keras.layers.Reshape((1,5)))
  model.add(keras.layers.Conv1D(filters=9, kernel_size=3, padding='same', activation='sigmoid'))
  model.add(keras.layers.MaxPooling1D(pool_size=3, strides=2, padding='same'))
  model.add(keras.layers.GRU(4, activation='sigmoid', recurrent_activation='sigmoid', return_sequences=True))
  model.add(keras.layers.GRU(4, activation='sigmoid', recurrent_activation='sigmoid'))
  #model.add(keras.layers.Dropout(0.7))
  model.add(keras.layers.Dense(1, activation='pow'))
  model.compile(optimizer=optimizer(), loss=custom_mse_tempered, metrics=None)
  return model

quint_model = KerasRegressor(build_fn=get_quint_model, epochs=60, batch_size=5)
history = quint_model.fit(X_train, y_train, validation_split=0.0)
y_test_quint_hat = quint_model.predict(X_test)
print(y_test)
print(y_test_quint_hat)
print(math.sqrt(mean_squared_error(y_test, y_test_quint_hat)))

"""## K-Fold"""

folds = [0, 0, 0, 0, 0, 0, 0]
folds[0] = pd.read_csv('~/Desktop/final_data/2a.csv')
folds[1] = pd.read_csv('~/Desktop/final_data/3a.csv')
folds[2] = pd.read_csv('~/Desktop/final_data/2b.csv')
folds[3] = pd.read_csv('~/Desktop/final_data/3b.csv')
folds[4] = pd.read_csv('~/Desktop/final_data/4a.csv')
folds[5] = pd.read_csv('~/Desktop/final_data/5a.csv')
folds[6] = pd.read_csv('~/Desktop/final_data/4b.csv')

print(folds)

rmses = list()

def reset_seed():
  import time
  t = 1000 * time.time()
  seed_value= int(t) % 2**32

  # 1. Set `PYTHONHASHSEED` environment variable at a fixed value
  import os
  os.environ['PYTHONHASHSEED']=str(seed_value)

  # 2. Set `python` built-in pseudo-random generator at a fixed value
  import random
  random.seed(seed_value)

  # 3. Set `numpy` pseudo-random generator at a fixed value
  import numpy as np
  np.random.seed(seed_value)

  # 4. Set the `tensorflow` pseudo-random generator at a fixed value
  import tensorflow as tf
  tf.random.set_seed(seed_value)
  # for later versions:
  # tf.compat.v1.set_random_seed(seed_value)

  # 5. Configure a new global `tensorflow` session
  from keras import backend as K
  session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)
  sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)
  tf.compat.v1.keras.backend.set_session(sess)

import copy
for i in range(len(folds)):
  test = folds[i]
  folds_e = copy.copy(folds)
  folds_e.pop(i)
  train = pd.concat(folds_e)
  X_test = test[features]
  y_test = test['remainingUsefulLifespan']
  X_train = train[features]
  y_train = train['remainingUsefulLifespan']
  reset_seed()
  model = KerasRegressor(build_fn=get_model, epochs=60, batch_size=5)
  history = model.fit(X_train, y_train, validation_split=0.0)
  y_test_hat = model.predict(X_test)
  rmses.append(math.sqrt(mean_squared_error(y_test, y_test_hat)))

print(rmses)

import statistics
print(statistics.mean(rmses))

print(statistics.stdev(rmses))